{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다중 선형 회귀(Multivariable Linear Regression)\n",
    "\n",
    "\n",
    "x가 1개인 선형 회귀를 **단순 선형 회귀(Simple Linear Regression)**라고 했다.\n",
    "\n",
    "y를 예측하는 x가 다수일 때 **다중 선형 회귀(Multivariable Linear Regression)**라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용 예제\n",
    "\n",
    "|**Quiz1**|**Quiz2**|**Quiz3**|**Final(y)**|\n",
    "|---|---|---|---|\n",
    "|73|80|75|152|\n",
    "|93|88|93|185|\n",
    "|89|91|80|180|\n",
    "|96|98|100|196|\n",
    "|73|66|70|142|\n",
    "\n",
    "다음과 같이 3개의 퀴즈 점수로 최종 점수를 예측하는 모델을 만들어 볼 계획이다.\n",
    "\n",
    "\n",
    "### $$H(x) = w_1x_1 + w_2x_2 + w_3x_3 + b$$\n",
    "**<center>독립 변수 x의 개수가 3개인 수식<center>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch로 직접 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn # 선형 회귀 불러오기 위해 사용\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터\n",
    "x1_train = torch.FloatTensor([[73],[93],[89],[86],[73]])\n",
    "x2_train = torch.FloatTensor([[80],[88],[91],[98],[66]])\n",
    "x3_train = torch.FloatTensor([[75],[93],[80],[100],[70]])\n",
    "y_train = torch.FloatTensor([[152],[185],[180],[196],[142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 w와 편향 b 설정, requires_grad = 기울기를 저장\n",
    "w1 = torch.zeros(1, requires_grad = True)\n",
    "w2 = torch.zeros(1, requires_grad = True)\n",
    "w3 = torch.zeros(1, requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:147: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:115.)\n",
      "  Variable._execution_engine.run_backward(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 w1: 0.286 w3: 0.294 b: 0.290 Cost: 0.003420\n",
      "Epoch  100/1000 w1: 0.670 w3: 0.695 b: 0.680 Cost: 0.007792\n",
      "Epoch  200/1000 w1: 0.667 w3: 0.700 b: 0.678 Cost: 0.007542\n",
      "Epoch  300/1000 w1: 0.665 w3: 0.705 b: 0.676 Cost: 0.007294\n",
      "Epoch  400/1000 w1: 0.662 w3: 0.709 b: 0.673 Cost: 0.007048\n",
      "Epoch  500/1000 w1: 0.660 w3: 0.713 b: 0.671 Cost: 0.006803\n",
      "Epoch  600/1000 w1: 0.658 w3: 0.718 b: 0.669 Cost: 0.006559\n",
      "Epoch  700/1000 w1: 0.656 w3: 0.721 b: 0.667 Cost: 0.006316\n",
      "Epoch  800/1000 w1: 0.654 w3: 0.725 b: 0.666 Cost: 0.006075\n",
      "Epoch  900/1000 w1: 0.652 w3: 0.729 b: 0.664 Cost: 0.005834\n",
      "Epoch 1000/1000 w1: 0.650 w3: 0.732 b: 0.662 Cost: 0.005595\n"
     ]
    }
   ],
   "source": [
    "# optimizer 설정, 학습률 0.00001\n",
    "optimizer = optim.SGD([w1,w2,w3, b], lr = 1e-5)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1) :\n",
    "    \n",
    "    # H(x) 계산\n",
    "    hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0 :\n",
    "        print('Epoch {:4d}/{} w1: {:.3f} w3: {:.3f} b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs,w1.item(),w2.item(),w3.item(),b.item(),cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.module을 사용하여 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 \n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 선언 및 초기화, 다중 선형 회귀 이므로 input_dim = 3, output_dim = 1\n",
    "model = nn.Linear(3,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3개의 입력 x에 대해 하나의 출력 y를 가지므로, torch.nn.Linear의 인자로 3,1을 사용했다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.5101, -0.5261,  0.4455]], requires_grad=True), Parameter containing:\n",
      "tensor([0.4149], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "# 3개의 w값과 b 가져오기\n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "처음 출력된 3개의 값이 w, 두번째 출력되는 것이 b의 값이다. 두 값 모두 현재는 랜덤 초기화가 되어져 있으며, 두 출력 결과 모두 학습의 대상이므로 **requires_grad = True**가 되어져 있는 것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 Cost: 18138.427734\n",
      "Epoch  100/2000 Cost: 3.483390\n",
      "Epoch  200/2000 Cost: 3.325043\n",
      "Epoch  300/2000 Cost: 3.174954\n",
      "Epoch  400/2000 Cost: 3.032701\n",
      "Epoch  500/2000 Cost: 2.897868\n",
      "Epoch  600/2000 Cost: 2.770078\n",
      "Epoch  700/2000 Cost: 2.648943\n",
      "Epoch  800/2000 Cost: 2.534109\n",
      "Epoch  900/2000 Cost: 2.425257\n",
      "Epoch 1000/2000 Cost: 2.322070\n",
      "Epoch 1100/2000 Cost: 2.224263\n",
      "Epoch 1200/2000 Cost: 2.131519\n",
      "Epoch 1300/2000 Cost: 2.043565\n",
      "Epoch 1400/2000 Cost: 1.960186\n",
      "Epoch 1500/2000 Cost: 1.881137\n",
      "Epoch 1600/2000 Cost: 1.806187\n",
      "Epoch 1700/2000 Cost: 1.735089\n",
      "Epoch 1800/2000 Cost: 1.667685\n",
      "Epoch 1900/2000 Cost: 1.603754\n",
      "Epoch 2000/2000 Cost: 1.543105\n"
     ]
    }
   ],
   "source": [
    "# 옵티마이저 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr = 1e-5)\n",
    "\n",
    "nb_epochs = 2000\n",
    "\n",
    "for epoch in range(nb_epochs + 1) :\n",
    "    \n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    # model(x_train)은 model.forward(x_train)와 동일함 \n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 MSE\n",
    "    \n",
    "    # cost로 H(x) 개선하는 부분\n",
    "    # gradient를 0으로 초기화\n",
    "    optimizer.zero_grad()\n",
    "    # 비용 함수를 미분하여 gradient 계산\n",
    "    cost.backward()\n",
    "    # w와 b를 업데이트\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0 :\n",
    "        # 100번마다 로그 출력\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "        epoch,nb_epochs, cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[149.5582]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 새로운 임의 데이터 생성\n",
    "new_var = torch.FloatTensor([[73,80,75]])\n",
    "# 임의 데이터 예측\n",
    "pred_y = model(new_var)\n",
    "print(pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.9223, 0.1582, 0.9221]], requires_grad=True), Parameter containing:\n",
      "tensor([0.4231], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "# 학습 후의 w와 b의 값을 출력\n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델을 클래스로 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다시 데이터 불러오기\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다중회귀 클래스 생성\n",
    "class MultivariateLinearRegressionModel(nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3,1) # 다중 선형회귀이므로 input_dim = 3, output_dim = 1\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model에 저장\n",
    "model = MultivariateLinearRegressionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 옵티마이저 생성\n",
    "optimizer = optim.SGD(model.parameters(), lr = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 Cost: 21379.408203\n",
      "Epoch  100/2000 Cost: 1.050733\n",
      "Epoch  200/2000 Cost: 1.007648\n",
      "Epoch  300/2000 Cost: 0.966829\n",
      "Epoch  400/2000 Cost: 0.928151\n",
      "Epoch  500/2000 Cost: 0.891486\n",
      "Epoch  600/2000 Cost: 0.856742\n",
      "Epoch  700/2000 Cost: 0.823806\n",
      "Epoch  800/2000 Cost: 0.792593\n",
      "Epoch  900/2000 Cost: 0.763001\n",
      "Epoch 1000/2000 Cost: 0.734959\n",
      "Epoch 1100/2000 Cost: 0.708375\n",
      "Epoch 1200/2000 Cost: 0.683164\n",
      "Epoch 1300/2000 Cost: 0.659287\n",
      "Epoch 1400/2000 Cost: 0.636633\n",
      "Epoch 1500/2000 Cost: 0.615163\n",
      "Epoch 1600/2000 Cost: 0.594798\n",
      "Epoch 1700/2000 Cost: 0.575500\n",
      "Epoch 1800/2000 Cost: 0.557194\n",
      "Epoch 1900/2000 Cost: 0.539848\n",
      "Epoch 2000/2000 Cost: 0.523391\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 2000\n",
    "\n",
    "for epoch in range(nb_epochs + 1) :\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    # model(x_train)은 model.forward(x_train)와 동일함\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 MSE 함수\n",
    "    \n",
    "    # cost로 H(x) 개선하는 부분\n",
    "    # gradient를 0으로 초기화\n",
    "    optimizer.zero_grad()\n",
    "    # 비용 함수를 미분하여 gradient 계산\n",
    "    cost.backward()\n",
    "    # w와 b를 업데이트\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0 :\n",
    "    # 100번마다 로그 출력\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "              epoch, nb_epochs, cost.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
