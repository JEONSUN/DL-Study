{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 전처리(Preprocessing)\n",
    "\n",
    "### Tokenizer() : 토큰화와 정수 인코딩(단어에 대한 인덱싱)을 위해 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "t = Tokenizer()\n",
    "fit_text = \"The earth is an awesome place live\"\n",
    "t.fit_on_texts([fit_text]) # fit_on_texts에 코퍼스를 입력하면 빈도수를 기준으로 단어 집합을 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences : [1, 2, 3, 4, 5, 6, 7]\n",
      "word_index : {'the': 1, 'earth': 2, 'is': 3, 'an': 4, 'awesome': 5, 'place': 6, 'live': 7}\n"
     ]
    }
   ],
   "source": [
    "test_text = \"The earth is an awesome place live\"\n",
    "sequences = t.texts_to_sequences([test_text])[0]\n",
    "\n",
    "print(\"sequences :\", sequences) \n",
    "print(\"word_index :\",t.word_index) # great는 단어 집합에 없으므로 출력되지 않는다. awesome :5만 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pad_sequence() :\n",
    "- 전체 훈련 데이터에서 각 샘플의 길이는 서로 다를 수 있다. 모델의 입력을 사용하려면 모든 샘플의 길이를 동일하게 맞춰야 할 때가 있는데(**Padding**), 보통 숫자 0을 넣어서 길이를 맞춘다.\n",
    "\n",
    "#### maxlen\n",
    "- 모든 데이터에 대해 정규화할 길이\n",
    "\n",
    "#### padding\n",
    "- 'pre'를 선택하면 앞에 0을 채우고 'post'를 선택하면 뒤에 0을 채움."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6],\n",
       "       [0, 7, 8]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "pad_sequences([[1,2,3],[3,4,5,6],[7,8]], maxlen = 3, padding = 'pre') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 워드 임베딩(Word Embedding)\n",
    "\n",
    "- 간단하게 말해 텍스트 내의 단어들을 밀집 벡터(dense vector)로 만드는 것을 말함\n",
    "\n",
    "- 원-핫 벡터가 대부분이 0의 값을 가지고 단 하나의 1의 값을 가지면서 벡터의 차원이 대체적으로 크다는 성질을 가졌다.\n",
    "> ex) [0 1 0 0 0 0 0 ..... 0 0 0 0] # 차원이 굉장히 크면서 대부분의 값이 0\n",
    "\n",
    "대부분의 값이 0인 이러한 벡터를 **희소 벡터(sparse vector)**이라고 하며, 원-핫 벡터는 희소 벡터의 예이다.\n",
    "\n",
    "원-학 벡터는 단어의 수만큼 벡터의 차원을 가지며 단어 간 유사도가 모두 동일하다는 단점이 있다.\n",
    "\n",
    "반면, 희소 벡터와 표기상으로도 의미상으로도 반대인 벡터가 있다. 대부분의 값이 실수이고, 상대적으로 저차원인 **밀집 벡터(dense vector)**이다\n",
    "> ex) [0.1 -1.2 0.2 1.8] # 상대적으로 저차원이며 실수값을 가짐\n",
    "\n",
    "|-|<center>원-핫 벡터<center>|<center>임베딩 벡터<center>|\n",
    "|---|---|---|\n",
    "|<center>차원<center>|<center>고차원(단어 집합의 크기)<center>|<center>저차원<center>|\n",
    "|<center>다른 표현<center>|<center>희소 벡터의 일종<center>|<center>밀집 벡터의 일종<center>|\n",
    "|<center>표현방법<center>|<center>수동<center>|<center>훈련 데이터로부터 학습함<center>|\n",
    "|<center>값의 타입|<center>1과 0<center>|<center>실수<center>|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어를 원-핫 벡터로 만드는 과정을 **원-핫 인코딩**이라고 함\n",
    "\n",
    "이와 대비하여 단어를 밀집 벡터로 만드는 작업을 **워드 임베딩(word embedding)**이라고 한다.\n",
    "\n",
    "밀집 벡터는 워드 임베딩 과정을 통해 나온 결과이므로 임베딩 벡터(embedding vector)라고도 부른다.\n",
    "\n",
    "원-핫 벡터의 차원이 주로 20000 이상을 넘어가는 것과는 달리 임베딩 벡터는 주로 256,512, 1024등의 차원을 가진다.\n",
    "\n",
    "임베딩 벡터는 초기에는 랜덤값을 가지지만, 인공 신경망의 가중치가 학습되는 방법과 같은 방식으로 값이 학습되며 변경된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2개의 문장 단어 토큰화\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "corpus1 = 'Hope to see you soon'\n",
    "corpus2 = 'Nice to see you again'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = [word_tokenize(corpus1),word_tokenize(corpus2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hope', 'to', 'see', 'you', 'soon'], ['Nice', 'to', 'see', 'you', 'again']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'to': 1, 'see': 2, 'you': 3, 'hope': 4, 'soon': 5, 'nice': 6, 'again': 7}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[4, 1, 2, 3, 5], [6, 1, 2, 3, 7]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 단어에 대한 정수 인코딩\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(text1)\n",
    "\n",
    "print(t.word_index)\n",
    "\n",
    "t.texts_to_sequences(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding() :\n",
    "\n",
    "**Embedding()**은 단어를 밀집 벡터로 만드는 역할을 한다. 인공 신경망 용어로는 임베딩 층(embedding layer)을 만드는 역할을 한다.\n",
    "\n",
    "먼저 **(number of samples, input_length)인 2D 정수 텐서**를 입력받는다. \n",
    "이 때 각 sample은 정수 인코딩이 된 결과로, 정수의 시퀀스를 의미한다. Embedding()은 워드 임베딩 작업을 수행하고 **(number of samples, input_length, embedding word dimensionality)인 3D 텐서를 리턴**한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.embeddings.Embedding at 0x223b6a12eb0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 위 데이터가 아래의 임베딩 층의 입력이 된다.\n",
    "Embedding(7,2, input_length = 5)\n",
    "# 7은 단어의 개수. 즉, 단어 집합(vocabulary)의 크기이다.\n",
    "# 2는 임베딩한 후의 벡터 크기이다.\n",
    "# 5는 각 입력 시퀀스의 길이. 즉, input_length이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 정수는 아래의 테이블의 인덱스로 사용되며 Embeddig()은 각 단어에 대해 임베딩 벡터를 리턴한다.\n",
    "\n",
    "|   index    | embedding  |\n",
    "|------------|------------|\n",
    "|     0      | [1.2, 3.1] |\n",
    "|     1      | [0.1, 4.2] |\n",
    "|     2      | [1.0, 3.1] |\n",
    "|     3      | [0.3, 2.1] |\n",
    "|     4      | [2.2, 1.4] |\n",
    "|     5      | [0.7, 1.7] |\n",
    "|     6      | [4.1, 2.0] |\n",
    "\n",
    "이때 위의 표는 임베딩 벡터가 된 결과를 예로 정리한 것이기에 Embedding()의 출력인 3D 텐서를 보여주는 것은 아니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 다층 퍼셉트론(MultiLayer Perceptron, MLP)로 텍스트 분류하기\n",
    "\n",
    "- 단층 퍼셉트론의 형태에서 은닉층이 1개 이상 추가된 신경망을 **다층 퍼셉트론(MLP)**라고 한다.\n",
    "\n",
    "다층 퍼셉트론은 **피드 포워드 신경망(Feed Forward Neural Network, FFNN)**의 가장 기본적인 형태며 피드 포워드 신경망은 입력층에서 출력층으로 오직 한 방향으로만 연산 방향이 정해져 있는 신경망을 말한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1. 케라스의 texts_to_matrix() 이해하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['먹고 싶은 사과','먹고 싶은 바나나','길고 노란 바나나 바나나',' 저는 과일이 좋아요']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'바나나': 1, '먹고': 2, '싶은': 3, '사과': 4, '길고': 5, '노란': 6, '저는': 7, '과일이': 8, '좋아요': 9}\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 데이터 정수 인코딩 수행\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(texts)\n",
    "print(t.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력된 텍스트 데이터로부터 행렬을 만들고자 할 때는 **texts_to_matrix()**를 사용한다. \n",
    "\n",
    "texts_to_matrix()는 총 4개의 모드를 지원하는데 각 모드는 'binary', 'count', 'freq', 'tfidf'로 총 4개이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 2. 0. 0. 0. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# 'count' 사용\n",
    "print(t.texts_to_matrix(texts, mode = 'count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count를 사용할 경우 **문서 단어 행렬(Document-Term Matrix, DTM)**을 생성한다.\n",
    "\n",
    "DTM의 인덱스는 앞서 확인한 word_index의 결과이다.\n",
    "이때, 주의할 점은 **각 단어에 부여되는 인덱스는 1부터 시작하는 반면에 완성되는 행렬의 인덱스는 0부터 시작한다**. \n",
    "> 실제로 단어의 개수는 9개였지만 완성된 행렬의 열 개수는 10개이며\n",
    ">\n",
    "> 첫번째 열은 그 어떤 단어도 할당되지 않았기 때문에 모든 행에서 값이 0인 것을 볼 수 있다.\n",
    "\n",
    "3번째 행에서는 {바나나:1}이 2번 등장했기 때문에 첫번째 열의 값은 2가 나온 것을 볼 수 있다.\n",
    "\n",
    "DTM은 bag of words를 기반으로 하므로 단어 순서 정보는 보존되지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# mode = 'binary'사용\n",
    "print(t.texts_to_matrix(texts, mode = 'binary'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DTM과 매우 유사해보이는 결과, 그러나 count와 달리 3번째 행의 첫번째 열의 값은 2가 아닌 1이 나왔다\n",
    "\n",
    "binary이기 때문에 해당 단어가 몇 개였는지는 무시하고 **존재하면 1, 아니면 0**의 값을 가지기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.   0.85 0.85 1.1  0.   0.   0.   0.   0.  ]\n",
      " [0.   0.85 0.85 0.85 0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   1.43 0.   0.   0.   1.1  1.1  0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   1.1  1.1  1.1 ]]\n"
     ]
    }
   ],
   "source": [
    "# mode = 'tfidf'\n",
    "print(t.texts_to_matrix(texts, mode = 'tfidf').round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tfidf는 말그대로 TF-IDF 행렬을 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.   0.33 0.33 0.33 0.   0.   0.   0.   0.  ]\n",
      " [0.   0.33 0.33 0.33 0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.5  0.   0.   0.   0.25 0.25 0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.33 0.33 0.33]]\n"
     ]
    }
   ],
   "source": [
    "# mode= 'freq'\n",
    "print(t.texts_to_matrix(texts, mode = 'freq').round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'freq'모드는 **각 문서에서의 각 단어의 등장 횟수를 분자로, 각 문서의 크기(각 문서에서 등장한 모든 단어의 개수의 총합)를 분모로 하는 표현 방법**이다.\n",
    "\n",
    "3번째 행을 예로들면 '길고 노란 바나나 바나나'에서 문서의 크기는 4인데 바나나는 총 2회 등장한다. 이에 따라 세번째 문장에서의 단어 **'바나나'**의 값은 위의 행렬에서 2/4 = 0.5가 된다.\n",
    "\n",
    "남은 **'길고'**, **'노란'**이라는 두 단어는 각 1회 등장하므로 각자 1/4의 값인 0.25를 가진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 20개 뉴스 그룹(Twenty Newsgroups) 데이터에 대한 이해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fetch_20newsgroups : 20개의 다른 주제를 가진 18,846개의 뉴스 그룹 이메일 데이터를 제공한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsdata = fetch_20newsgroups(subset= 'train') # 'train'을 기재하면 훈련 데이터만 리턴된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
     ]
    }
   ],
   "source": [
    "print(newsdata.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 샘플의 개수 : 11314\n"
     ]
    }
   ],
   "source": [
    "print(\"훈련용 샘플의 개수 : {}\".format(len(newsdata.data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 주제의 개수 : 20\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(\"총 주제의 개수 : {}\".format(len(newsdata.target_names)))\n",
    "print(newsdata.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트 데이터에서 이메일 본문을 보고 20개의 주제 중 어떤 주제인지를 맞추는 실습을 해보고자 한다.\n",
    "\n",
    "레이블인 target에는 총 0부터 19까지의 숫자가 들어가있는데 첫번째 샘플의 경우에는 몇 번 주제인지 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫번째 샘플의 레이블 : 7\n"
     ]
    }
   ],
   "source": [
    "print(\"첫번째 샘플의 레이블 : {}\".format(newsdata.target[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7번 레이블이 의미하는 주제 : rec.autos\n"
     ]
    }
   ],
   "source": [
    "print(\"7번 레이블이 의미하는 주제 : {}\".format(newsdata.target_names[7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 첫번째 이메일의 내용 출력\n",
    "print(newsdata.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               email  target\n",
       "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...       7\n",
       "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...       4\n",
       "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...       4\n",
       "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...       1\n",
       "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...      14"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 메일본문 data와 레이블인 target을 데이터프레임으로 만들어서 데이터에 대한 통계적인 정보 출력\n",
    "data = pd.DataFrame(newsdata.data, columns = ['email'])\n",
    "data['target'] = pd.Series(newsdata.target)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11314 entries, 0 to 11313\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   email   11314 non-null  object\n",
      " 1   target  11314 non-null  int32 \n",
      "dtypes: int32(1), object(1)\n",
      "memory usage: 132.7+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# null값 유무 확인\n",
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복을 제외한 샘플의 수 : 11314\n",
      "중복을 제외한 주제의 수 : 20\n"
     ]
    }
   ],
   "source": [
    "# nunique를 통해 샘플 중 중복을 제거한 개수를 확인할 수 있다.\n",
    "print('중복을 제외한 샘플의 수 : {}'.format(data['email'].nunique()))\n",
    "print('중복을 제외한 주제의 수 : {}'.format(data['target'].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['target'].value_counts().plot(kind = 'bar')\n",
    "plt.xticks(rotation = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10번 레이블의 수가 가장 많고, 19번 레이블의 수가 가장 적으며 대체적으로 400~600개 사이의 분포를 보인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsdata_test = fetch_20newsgroups(subset = 'test', shuffle = True) # test를 기재하면 테스트 데이터만 리턴한다.\n",
    "train_email = data['email'] # 훈련 데이터만 본문 저장 \n",
    "train_label = data['target'] # 훈련 데이터에 레이블 저장\n",
    "test_email = newsdata_test.data # 테스트 데이터의 본문 저장 \n",
    "test_label = newsdata_test.target # 테스트 데이터의 라벨 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 10000 # 학습에 사용할 단어의 최대 개수\n",
    "num_classes = 20 # 레이블의 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(train_data, test_data, mode) : # 전처리 함수\n",
    "    t = Tokenizer(num_words = max_words) # max_words 개수만큼의 단어를 사용한다.\n",
    "    t.fit_on_texts(train_data)\n",
    "    X_train = t.texts_to_matrix(train_data, mode = mode) # 샘플 수 x max_words 크기의 행렬 생성, 모드는 사용자가 정의\n",
    "    X_test = t.texts_to_matrix(test_data, mode = mode) # 샘플 수 x max_words 크기의 행렬 생성\n",
    "    return X_train, X_test, t.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, index_to_word = prepare_data(train_email, test_email, 'binary')\n",
    "y_train = to_categorical(train_label, num_classes) # 원핫인코딩\n",
    "y_test = to_categorical(test_label, num_classes) # 원핫인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플 본문의 크기 : (11314, 10000)\n",
      "훈련 샘플 레이블의 크기 : (11314, 20)\n",
      "테스트 샘플 본문의 크기 : (7532, 10000)\n",
      "테스트 샘플 레이블의 크기 : (7532, 20)\n"
     ]
    }
   ],
   "source": [
    "print('훈련 샘플 본문의 크기 : {}'.format(X_train.shape))\n",
    "print('훈련 샘플 레이블의 크기 : {}'.format(y_train.shape))\n",
    "print('테스트 샘플 본문의 크기 : {}'.format(X_test.shape))\n",
    "print('테스트 샘플 레이블의 크기 : {}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 샘플은 총 11314개 x 10000 이때, 열이 10000인 이유는 max_words(10000개)를 num_words로 지정했기 때문이다. 단어의 정수 인덱스는 1, 행렬 인덱스는 0부터 시작이므로 사실상 9999개 단어를 사용했다고 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "빈도수 상위 1번 단어 : the\n",
      "빈도수 상위 9999번 단어 : mic\n"
     ]
    }
   ],
   "source": [
    "print('빈도수 상위 1번 단어 : {}'.format(index_to_word[1]))\n",
    "print('빈도수 상위 9999번 단어 : {}'.format(index_to_word[9999]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다층 퍼셉트론(Multilayer Perceptron, MLP)을 사용하여 텍스트 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 설계에 필요한 라이브러리\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_evaluate(X_train, y_train, X_test, y_test):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_shape=(max_words,), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, batch_size=128, epochs=5, verbose=1, validation_split=0.1)\n",
    "    score = model.evaluate(X_test, y_test, batch_size=128, verbose=0)\n",
    "    return score[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "설계한 신경망은 총 4개의 층을 가지고 있다.\n",
    "\n",
    "> **max_words(10000)의 크기를 가진 입력층**\n",
    "\n",
    "> **256개의 뉴련을 가진 첫번째 은닉층**\n",
    "\n",
    "> **128개의 뉴런을 가진 두번째 은닉층**\n",
    "\n",
    "> **num_classes(20)의 크기를 가진 출력층**\n",
    "\n",
    "이번에 설계한 다층 퍼셉트론은 은닉층이 2개이므로 깊은 신경망(Deep Neural Network, DNN)이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "80/80 [==============================] - 3s 32ms/step - loss: 2.3535 - accuracy: 0.3178 - val_loss: 1.0226 - val_accuracy: 0.8242\n",
      "Epoch 2/5\n",
      "80/80 [==============================] - 2s 28ms/step - loss: 0.9053 - accuracy: 0.7496 - val_loss: 0.4641 - val_accuracy: 0.8905\n",
      "Epoch 3/5\n",
      "80/80 [==============================] - 2s 27ms/step - loss: 0.4530 - accuracy: 0.8790 - val_loss: 0.3501 - val_accuracy: 0.8975\n",
      "Epoch 4/5\n",
      "80/80 [==============================] - 2s 27ms/step - loss: 0.2626 - accuracy: 0.9323 - val_loss: 0.3100 - val_accuracy: 0.9117\n",
      "Epoch 5/5\n",
      "80/80 [==============================] - 2s 26ms/step - loss: 0.1710 - accuracy: 0.9586 - val_loss: 0.2954 - val_accuracy: 0.9152\n",
      "binary 모드의 테스트 정확도: 0.8308550119400024\n",
      "Epoch 1/5\n",
      "80/80 [==============================] - 3s 29ms/step - loss: 2.6782 - accuracy: 0.2628 - val_loss: 1.5126 - val_accuracy: 0.7588\n",
      "Epoch 2/5\n",
      "80/80 [==============================] - 2s 26ms/step - loss: 1.3514 - accuracy: 0.6432 - val_loss: 0.6865 - val_accuracy: 0.8436\n",
      "Epoch 3/5\n",
      "80/80 [==============================] - 2s 26ms/step - loss: 0.7793 - accuracy: 0.8076 - val_loss: 0.5528 - val_accuracy: 0.8666\n",
      "Epoch 4/5\n",
      "80/80 [==============================] - 2s 30ms/step - loss: 0.5148 - accuracy: 0.8715 - val_loss: 0.4550 - val_accuracy: 0.8896\n",
      "Epoch 5/5\n",
      "80/80 [==============================] - 2s 29ms/step - loss: 0.3870 - accuracy: 0.9085 - val_loss: 0.4062 - val_accuracy: 0.9028\n",
      "count 모드의 테스트 정확도: 0.8220924139022827\n",
      "Epoch 1/5\n",
      "80/80 [==============================] - 3s 30ms/step - loss: 2.2182 - accuracy: 0.3672 - val_loss: 0.7758 - val_accuracy: 0.8481\n",
      "Epoch 2/5\n",
      "80/80 [==============================] - 2s 28ms/step - loss: 0.8475 - accuracy: 0.7687 - val_loss: 0.4179 - val_accuracy: 0.8949\n",
      "Epoch 3/5\n",
      "80/80 [==============================] - 2s 27ms/step - loss: 0.4580 - accuracy: 0.8845 - val_loss: 0.3345 - val_accuracy: 0.9134\n",
      "Epoch 4/5\n",
      "80/80 [==============================] - 2s 28ms/step - loss: 0.2881 - accuracy: 0.9289 - val_loss: 0.3153 - val_accuracy: 0.9187\n",
      "Epoch 5/5\n",
      "80/80 [==============================] - 2s 29ms/step - loss: 0.2052 - accuracy: 0.9500 - val_loss: 0.2998 - val_accuracy: 0.9205\n",
      "tfidf 모드의 테스트 정확도: 0.828066885471344\n",
      "Epoch 1/5\n",
      "80/80 [==============================] - 3s 32ms/step - loss: 2.9819 - accuracy: 0.0770 - val_loss: 2.9445 - val_accuracy: 0.1943\n",
      "Epoch 2/5\n",
      "80/80 [==============================] - 2s 28ms/step - loss: 2.7550 - accuracy: 0.2091 - val_loss: 2.4243 - val_accuracy: 0.3481\n",
      "Epoch 3/5\n",
      "80/80 [==============================] - 2s 30ms/step - loss: 2.1951 - accuracy: 0.3206 - val_loss: 1.8822 - val_accuracy: 0.5530\n",
      "Epoch 4/5\n",
      "80/80 [==============================] - 2s 30ms/step - loss: 1.7545 - accuracy: 0.4533 - val_loss: 1.4863 - val_accuracy: 0.6873\n",
      "Epoch 5/5\n",
      "80/80 [==============================] - 2s 30ms/step - loss: 1.4006 - accuracy: 0.5776 - val_loss: 1.1727 - val_accuracy: 0.7288\n",
      "freq 모드의 테스트 정확도: 0.6711364984512329\n"
     ]
    }
   ],
   "source": [
    "modes = ['binary', 'count', 'tfidf', 'freq'] # 4개의 모드를 리스트에 저장.\n",
    "\n",
    "for mode in modes: # 4개의 모드에 대해서 각각 아래의 작업을 반복한다.\n",
    "    X_train, X_test, _ = prepare_data(train_email, test_email, mode) # 모드에 따라서 데이터를 전처리\n",
    "    score = fit_and_evaluate(X_train, y_train, X_test, y_test) # 모델을 훈련하고 평가.\n",
    "    print(mode+' 모드의 테스트 정확도:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'freq'의 정확도가 혼자 0.67을 웃돌았다. 적절한 전처리가 아니었다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
