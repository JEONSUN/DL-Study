{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 통계 기반 언어 모델(Statistic Language Model,  SLM)\n",
    "\n",
    "\n",
    "**언어 모델(language model)**이란 단어 시퀀스에 확률(probability)을 부여(assign)하는 모델이다.\n",
    "\n",
    "단어의 등장 순서를 무시하는 백오브워즈(Bow)와는 달리 언어 모델은 시퀀스 정보를 명시적으로 학습한다.\n",
    "\n",
    "따라서 백오브워즈의 대척점에 언어 모델이 있다고 말할 수 있다.\n",
    "\n",
    "단어가 n개 주어진 상황이라면 언어 모델은 n개 단어가 동시에 나타날 확률, 즉 $P(w_1,...,w_n)$을 반환한다.\n",
    "\n",
    "통계 기반의 언어 모델은 말뭉치에서 해당 단어 시퀀스가 얼마나 자주 등장하는지 빈도를 세어 학습한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. 조건부 확률\n",
    "\n",
    "조건부 확률은 두 확률 $P(A), P(B)에 대해서 다음과 같은 관계를 갖는다.\n",
    "\n",
    "### $$P(B/A) = P(A,B)/P(A)$$\n",
    "\n",
    "### $$P(A,B) = P(A)P(B|A)$$\n",
    "\n",
    "더 많은 확률에 대해 일반화해보자. 4개의 확률이 조건부 확률의 관계를 가질 때, 아래와 같이 표현할 수 있다.\n",
    "\n",
    "### $$P(A,B,C,D) = P(A)P(B|A)P(C|A,B)P(D|A,B,C)$$\n",
    "\n",
    "이를 조건부 확률의 **연쇄 법칙(chain rule)**이라고 한다. 이제는 4개가 아닌 n개에 대해 일반화를 해보자.\n",
    "\n",
    "### $$P(x_1,x_2,x_3...x_n) = P(x_1)P(x_2|x_1)P(x_3|x_1,x_2)...P(x_n|x_1...x_{n-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 문장에 대한 확률\n",
    "\n",
    "문장 'An adorable little boy is spreading smiles'의 확률 **P(An adorable little boy is spreading smiles)**를 식으로 표현해보자.\n",
    "\n",
    "각 단어는 문맥이라는 관계로 인해 이전 단어의 영향을 받아 나온 단어들이다. 그리고 모든 단어로부터 하나의 문장이 완성된다. 그렇기 때문에 문장의 확률을 구하고자 조건부 확률을 사용하겠다. \n",
    "\n",
    "문장의 확률은 각 단어들이 이전 단어가 주어졌을 때 다음 단어로 등장할 확률의 곱으로 구성된다.\n",
    "문장에 해당 식을 적용해보면 다음과 같다.\n",
    "\n",
    "#### $P(An \\ adorable \\ little \\ boy \\ is \\ spreading \\ smiles) =  \\\\\n",
    "P(AN) \\times P(adorable|An) \\times P(little|An \\ adorable) \\times P(boy|An \\ adorable \\ little) \\times P(is|An \\ adorable \\ little \\ boy) \\times P(spreading|An \\ adorable \\ little \\ boy \\ is) \\times P(smiles|An \\ adorable \\ little \\ is \\ spreading) $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 카운트 기반의 접근\n",
    "\n",
    "문장의 확률을 구하기 위해서 각 단어에 대한 예측 확률들을 곱한다는 것을 알았다. 그렇다면 SLM은 이전 단어로부터 다음 단어에 대한 확률은 어떻게 구할까? 정답은 카운트에 기반하여 확률을 계산하는 것이다.\n",
    "\n",
    "An adorable little boy가 나왔을 때 is가 나올 확률인 $P(is|An \\ adorable \\ little \\ boy)$를 구해보자\n",
    "\n",
    "$$ P(is|An \\ adorable \\ little \\ boy) = count(An \\ adorable \\ little \\ boy \\ is) \\over count(An \\ adorable \\ little \\ boy)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 확률은 위와 같다. 예를 들어 기계가 학습한 코퍼스 데이터에서 An adorable little boy가 100번 등장했는데 그 다음에 is가 등장한 경우는 30번이라고 한다면 $P(is|An \\ adorable \\ little \\ boy)$는 30%이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 카운트 기반 접근의 한계 - 희소 문제(Sparsity Problem)\n",
    "\n",
    "기계에서 많은 코퍼스를 훈련시켜서 언어 모델을 통해 현실에서의 확률 분포를 근사하는 것이 언어 모델의 목표이다. 그런데 카운트 기반으로 접근하려고 한다면 갖고있는 코퍼스(corpus)의 양(기계가 학습하는 데이터)는 정말 많은 양이 필요할 것이다.\n",
    "\n",
    "만약 $P(is|An \\ adorable \\ little \\ boy)$를 구하는 경우에서 훈련한 코퍼스에 'An adorable little boy is'라는 단어 시퀀스가 없었다면 이 단어 시퀀스에 대한 확률은 0이 된다. 또는 An adorable little boy라는 단어 시퀀스가 없었다면 분모가 0이 되어 확률은 정의되지 않는다.\n",
    "\n",
    "이와 같이 충분한 데이터를 관측하지 못하여 언어를 정확히 모델링하지 못하는 문제를 **희소 문제(Sparsity problem)**라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-gram\n",
    "\n",
    "SLM의 한계는 훈련 코퍼스 훈련 코퍼스에 확률을 계산하고 싶은 문장이나 단어가 없을 수 있다는 점이다. \n",
    "그리고 확률을 계산하고 싶은 문장이 길어질수록 갖고있는 코퍼스에서 그 문장이 존재하지 않을 가능성이 높다.\n",
    "\n",
    "다시 말해 카운트할 수 없을 가능성이 높은데, 단어들을 줄이면 카운트를 할 수 있을 가능성을 높일 수 있다.\n",
    "\n",
    "$P(is|An \\ adorable \\ little \\ boy)$ ==> $P(is|boy)$\n",
    "\n",
    "긴 코퍼스 대신 boy 다음 is가 나온다고 가정하기\n",
    "\n",
    "$P(is|An \\ adorable \\ little \\ boy)$ ==> $P(is|little boy)$\n",
    "\n",
    "little boy 다음 boy가 나온다고 가정하기\n",
    "\n",
    "\n",
    "이렇듯 단어의 확률을 구하고자 기준 단어의 앞 단어를 전부 포함해서 카운트하는 것이 아닌, 앞 단어 중 임의의 개수만 포함해서 카운트하여 근사하자는 것을 의미한다.\n",
    "이렇게 하면 갖고 있는 코퍼스에서 해당 단어의 시퀀스를 카운트할 확률이 높아진다.\n",
    "\n",
    "이 때 임의의 개수를 정하기 위한 기준을 위해 사용하는 것이 **n-gram**이다. n-gram은 n개의 연속적인 단어 나열을 의미하며, 갖고 있는 코퍼스에서 n개의 단어 뭉치 단위로 끊어서 이를 하나의 토큰으로 간주한다.\n",
    "\n",
    "\n",
    "\n",
    "문장 An adorable little boy is spreading smiles이 있을 때, 각 n에 대한 n-gram들\n",
    "\n",
    "### unigrams \n",
    "> **n이 1일 때**<br>\n",
    "> an, adorable, little, boy, is, spreading, smiles\n",
    "\n",
    "### bigrams\n",
    "> **n이 2일 때**<br>\n",
    "> an adorable, adorable little, little boy, boy is, is spreading, spreading smiles\n",
    "\n",
    "### trigrams\n",
    "> **n이 3일 때**<br>\n",
    "> an adorable little, adorable little boy, little boy is, boy is spreading, is spreading smiles\n",
    "\n",
    "### 4-grams\n",
    "> **n이 4일 때**, 4이상인 경우 앞에 숫자를 붙여 명명한다.(1-gram,2-gram,3-gram이라고 사용하기도 한다)<br>\n",
    "> an adorable little boy, adorable little boy is, little boy is spreading, boy is spreading smiles\n",
    "\n",
    "\n",
    "\n",
    "n-gram을 통한 언어 모델에서는 다음에 나올 단어의 예측은 오직 n-1개의 단어에만 의존한다. \n",
    "\n",
    "예를들어 'An adorable little boy is spreading' 다음에 나올 단어를 예측하고 싶다고 할 때, 4-gram에서 spreading 다음에 올 단어를 예측하는 것은 n-1에 해당되는 앞의 3개의 단어만을 고려한다.\n",
    "\n",
    "\n",
    "$$P(w|boy \\ is \\ spreading) = count(boy \\ is \\ spreading \\ w) \\over count(boy \\ is \\ spreading)$$\n",
    "\n",
    "\n",
    "만약 갖고있는 코퍼스에서 boy is spreading이 1000번 등장하고, boy is spreading insults가 500번, boy is spreading smiles가 200번 등장했다고 생각해보자.\n",
    "\n",
    "boy is spreading 다음 insults가 등장할 확률은 500/1000 = 0.5이며, smiles가 등장할 확률은 200/1000 = 0.2이다. 따라서 insults가 더 맞다고 판단하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram Language Model의 한계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-gram은 뒤의 단어 몇 개만 보다 보니 의도하고 싶은 대로 문장을 끝맺음하지 못하는 경우가 생기게 된다.\n",
    "\n",
    "문장을 읽다 보면 앞 부분과 뒷부분의 문맥이 전혀 연결 안되는 경우도 생기는데 결국 전체 문장을 고려한 언어 모델보다는 정확도가 떨어질 수 밖에 없다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
