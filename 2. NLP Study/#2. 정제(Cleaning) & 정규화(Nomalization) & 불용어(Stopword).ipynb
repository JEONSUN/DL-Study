{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코퍼스에서 용도에 맞게 토큰을 분류하는 작업을 토큰화(tokenization)라고 하며, 토큰화 작업 전, 후에는 텍스트 데이터를 용도에 맞게 **정제(cleaning) 및 정규화(nomalization)**작업을 함께하게 된다.\n",
    "\n",
    "***\n",
    "- 정제(cleaning) : 갖고 있는 코퍼스로부터 노이즈 데이터를 제거한다.\n",
    "\n",
    "- 정규화(normalization) : 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들어준다.\n",
    "***\n",
    "\n",
    "정제 작업은 토큰화 작업에 방해가 되는 부분들을 배제시키고 토큰화 작업을 수행하기 위해서 토큰화 작업보다 앞서 이루어지기도 하지만, 토큰화 작업 이후에도 여전히 남아있는 노이즈들을 제거하기위해 지속적으로 이루어지기도 한다.\n",
    "\n",
    "사실, 완벽한 정제 작업은 어려운 편이라, 대부분의 경우 일종의 합의점을 찾기도 한다.\n",
    "\n",
    "***\n",
    "### 어간 추출(stemming) and 표제어 추출(Lemmatization)\n",
    "\n",
    "**USA와 US는 같은 의미를 가지므로, 하나의 단어로 정규화할 수 있다.**\n",
    "\n",
    "이렇듯, 같은 의미를 갖고 있음에도, 표기가 다른 단어들을 하나의 단어로 정규화하는 방법으로 어간 추출과 표제어 추출을을 활용하여 표기가 다른 단어들을 통합한다.\n",
    "\n",
    "겉으로 봤을때는 다른 단어여도 의미가 같은 하나의 단어로 일반화가 가능하다면,\n",
    "하나의 단어로 일반화시켜서 문서 내의 단어 수를 줄이는 것을 목적으로 둔다.\n",
    "\n",
    "자연어 처리에서의 전처리, **정규화의 지향점은 언제나 갖고 있는 코퍼스로부터 복잡성을 줄이는 일이다.**\n",
    "\n",
    "***\n",
    "\n",
    "### 1. 표제어 추출(Lemmatization)\n",
    "\n",
    "표제어(Lemma)는 한글로는 '표제어' 또는 '기본 사전형 단어'정도의 의미를 갖는데, **표제어 추출**은 단어들로부터 표제어를 찾아가는 과정이다. 표제어 추출은 단어들이 다른 형태를 가지더라도, 그 뿌리 단어를 찾아 단어의 개수를 줄일 수 있는지 판단한다.\n",
    "\n",
    "> **am, are, is ------> be** : be는 세 단어의 뿌리 단어이자 표제어가 된다.\n",
    "\n",
    "표제어 추출을 하는 가장 섬세한 방법은 단어의 형태학적 파싱을 먼저 진행하는 것인데,\n",
    "이때 **형태소란 의미를 가진 가장 작은 단위**를 뜻하며, 형태학이란 형태소로부터 단어들을 만들어가는 학문을 뜻한다.\n",
    "\n",
    "형태소는 두 가지 종류가 있다.\n",
    "***\n",
    "> **1) 어간(stem)**\n",
    "> : 단어의 의미를 담고 있는 단어의 핵심 부분\n",
    "\n",
    "> **2) 접사(affix)**\n",
    "> : 단어에 추가적인 의미를 주는 부분\n",
    "***\n",
    "\n",
    "형태학적 파싱은 이 두가지 구성요소를 분리하는 작업을 말한다.\n",
    "\n",
    "예를들어 cats라는 단어에 대해 형태학적 파싱을 수행한다면 cat(어간)과 -s(접사)를 분리한다. 그렇다고 반드시 2가지 구성요소로 분리되는 것도 아닌데 fox라는 단어를 형태학적 파싱한다면 fox는 독립적인 형태소 이므로 더 이상 분리가 불가능하다.\n",
    "\n",
    "**NLTK에서는 표제어 추출을 위한 도구로 WordNetLemmatizer를 지원한다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['named', 'done', 'watched', 'fish', 'ate', 'ha', 'starting', 'food']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "n = WordNetLemmatizer()\n",
    "words = ['named', 'done', 'watched', 'fishes','ate', 'has', 'starting','foods']\n",
    "\n",
    "# 표제어 추출\n",
    "print([n.lemmatize(w) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "표제어 추출은 단어의 형태가 적절히 보존되는 양상을 보이는데, ha같이 의미를 알 수 없는 적절하지 못한 단어를 출력하고 있다.\n",
    "\n",
    "이는 표제어 추출기(lemmatizer)가 본래 단어의 품사 정보를 알아야만 정확한 결과를 얻을 수 있기 때문이다.\n",
    "\n",
    "따라서 WordNetLemmatizer는 입력으로 단어가 동사 품사라는 사실을 알려줄 수 있다.\n",
    "즉, **특정 단어가 문장에서 동사로 쓰였다는 것을 알려준다면** 표제어 추출기는 품사의 정보를 보존하면서 정확한 Lemma를 출력하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'do'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.lemmatize('done','v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'watch'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.lemmatize('watched','v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.lemmatize('has','v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "표제어 추출은 문맥을 고려하며, 수행했을 때의 결과는 해당 단어의 품사 정보를 보존한다.(POS 태그를 보존한다고도 말할 수 있다.)\n",
    "\n",
    "하지만 어간 추출을 수행한 결과는 품사 정보가 보존되지 않는다(POS 태그를 고려하지 않는다.) 더 정확히는, 어간 추출을 한 결과는 사전에 존재하지 않을 경우가 많다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 2. 어간 추출(Stemming)\n",
    "\n",
    "**어간(stem)을 추출하는 작업을 어간 추출(stemming)**이라고 한다.\n",
    "어간 추출은 형태학적 분석을 단순화한 버전이라고 볼 수도 있고, 정해진 규칙만 보고 단어의 의미를 자르는 어림짐작의 작업이라고 볼 수도 있다.\n",
    "\n",
    "이 작업은 섬세한 작업이 아니기 때문에 어간 추출 후에 나오는 결과 단어는 사전에 존재하지 않는 단어일 수도 있다.\n",
    "\n",
    "예시로 살펴보자. 어간 추출 알고리즘 중 하나인 **포터 알고리즘(Porter Algorithm)**에 아래의 텍스트를 입력으로 넣었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "s = PorterStemmer()\n",
    "text = \"This was not the map we found in Billy Bones's chest, \\\n",
    "but an accurate copy, \\\n",
    "complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
    "\n",
    "words = word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
     ]
    }
   ],
   "source": [
    "print([s.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 알고리즘의 결과에는 사전에 없는 단어들도 포함되어 있다. 위의 어간 추출은 단순 규칙에 기반하여 이루어지기 때문이다.\n",
    "\n",
    "포터 알고리즘의 어간 추출은 이러한 규칙들을 가진다.... 라는데\n",
    "\n",
    "좀더 간단한 예시를 찾아봤다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "pst = PorterStemmer()\n",
    "print(pst.stem('doing'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming**이란 어간추출을 의미하며, **한마디로 형태가 변한 단어로부터 그 단어의 원형을 추출하는 것이다.**\n",
    "\n",
    "그래서 **'doing'**을 stemming할 시 **'do'**를 추출하고 **'crosses'**를 stemming할 시 **'cross'**를 추출한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing cross\n"
     ]
    }
   ],
   "source": [
    "# lemmatization 수행시\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "n = WordNetLemmatizer()\n",
    "print(n.lemmatize('doing'),n.lemmatize('crosses'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 불용어(Stopword)\n",
    "\n",
    "갖고 있는 데이터에서 유의미한 단어 토큰만을 선별하기 위해서는 큰 의미가 없는 단어 토큰을 제거하는 작업이 필요하다.\n",
    "\n",
    "다시 말해 **불용어**는 문장에 자주 등장하지만 분석을 하는 것에 있어서는 큰 도움이 되지 않는 단어들을 말한다. ex) i, my, me, 는, 등등...\n",
    "\n",
    "NLTK에서는 100개 이상의 영어 단어들을 불용어로 패키지 내에서 미리 정의해주는데, 개발자가 직접 정의할 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk가 정의한 영어 불용어 리스트 10개\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK를 통해서 불용어 제거하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'always', 'do', 'my', 'best', '....', 'to', 'improve', 'my', 'knowledge', '.']\n",
      "['I', 'always', 'best', '....', 'improve', 'knowledge', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example = 'I always do my best.... to improve my knowledge.'\n",
    "\n",
    "# 불용어 불러오기 \n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# 토큰화 진행\n",
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "result = []\n",
    "\n",
    "for w in word_tokens :\n",
    "    if w not in stop_words :\n",
    "        result.append(w)\n",
    "        \n",
    "print(word_tokens)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'do', 'my', 'to'와 같은 단어들이 제거되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 한국어에서 불용어 제거하기\n",
    "\n",
    "한국어에서 불용어를 제거하는 방법으로는 간단하게는 토큰화 후에 조사, 접속사 등을 제거하는 방법이 있다.<br>\n",
    "하지만 불용어 제거를 진행하다보면 명사, 형용사와 같은 단어들중에서도 불용어로 제거하고 싶은 단어들도 있기 마련이고, 결국 사용자가 직접 불용어 사전을 만들게 되는 경우가 많다.<br><br> 이번에는 직접 불용어를 정의해보고, 정의한 불용어 사전을 참고로 불용어를 제거해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['어릴', '적', ',', '줄', '서는', '것부터', '가르쳐', '준', '이유', '이젠', '선명해졌어.복잡한', '인간관계', ',', '그', '자체가', '역설.관계만', '있고', '인간이', '낄', '틈', '하나', '없어.평범해지는', '게', '두려워서', '꾸던', '꿈.이젠', '평범한', '게', '부럽군', '.']\n",
      "['어릴', '적', ',', '줄', '서는', '것부터', '가르쳐', '이유', '선명해졌어.복잡한', '인간관계', ',', '자체가', '역설.관계만', '있고', '인간이', '낄', '틈', '없어.평범해지는', '두려워서', '꾸던', '꿈.이젠', '평범한', '부럽군', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# 예시로 가져온 노래 가사\n",
    "text = \"어릴 적, 줄 서는 것부터 가르쳐 준 이유 이젠 선명해졌어.\\\n",
    "복잡한 인간관계, 그 자체가 역설.\\\n",
    "관계만 있고 인간이 낄 틈 하나 없어.\\\n",
    "평범해지는 게 두려워서 꾸던 꿈.이젠 평범한 게 부럽군.\"\n",
    "\n",
    "# 임의로 정의한 불용어\n",
    "stop_words = '이젠 그 준 하나 빈차 게 갈길 무게 달라'\n",
    "\n",
    "stop_words = stop_words.split(' ')\n",
    "\n",
    "word_tokens = word_tokenize(text)\n",
    "\n",
    "result = []\n",
    "for w in word_tokens :\n",
    "    if w not in stop_words :\n",
    "        result.append(w)\n",
    "        \n",
    "# 리스트 컴프리헨션 사용 가능\n",
    "# result = [word for word in word_tokens if not word in stop_words]\n",
    "\n",
    "print(word_tokens)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보편적으로 선택할 수 있는 한국어 불용어 리스트(참고용) : https://www.ranks.nl/stopwords/korean,  https://bab2min.tistory.com/544\n",
    "\n",
    "\n",
    "한국어 불용어를 제거하는 더 좋은 방법은 txt 파일이나 csv 파일로 수많은 불용어를 정리해놓고, 이를 불러와서 사용하는 방법이다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
