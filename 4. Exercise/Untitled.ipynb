{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integer Encoding\n",
    "\n",
    "- 단어 토큰화 또는 형태소 토큰화를 수행했다면 각 단어에 고유한 정수를 부여한다.\n",
    "\n",
    "- 이때 중복은 허용하지 않는다.\n",
    "\n",
    "- 중복이 허용되지 않는 모든 단어들의 집합을 단어 집합(Vocabulary)이라고 한다.\n",
    "\n",
    "참고 : https://wikidocs.net/31766"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"A barber is a person. a barber is good person. a barber is huge person. \\\n",
    "        he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. \\\n",
    "        His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. \\\n",
    "        the barber went up a huge mountain.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A barber is a person.', 'a barber is good person.', 'a barber is huge person.', 'he Knew A Secret!', 'The Secret He Kept is huge secret.', 'Huge secret.', 'His barber kept his word.', 'a barber kept his word.', 'His barber kept his secret.', 'But keeping and keeping such a huge secret to himself was driving the barber crazy.', 'the barber went up a huge mountain.']\n"
     ]
    }
   ],
   "source": [
    "# nltk를 활용하여 문장 토큰화\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text = sent_tokenize(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
     ]
    }
   ],
   "source": [
    "for i in text :\n",
    "    sentence = word_tokenize(i) # 단어 토큰화\n",
    "    result = []\n",
    "    \n",
    "    for word in sentence :\n",
    "        word = word.lower() # 모든 단어를 소문자화하여 단어의 개수를 줄임\n",
    "        if word not in stop_words : # 불용어 제거\n",
    "            if len(word) > 2 : # 단어 길이가 2이하인 경우에 추가로 단어 제거\n",
    "                result.append(word)\n",
    "    sentences.append(result)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['barber', 'person', 'barber', 'good', 'person', 'barber', 'huge', 'person', 'knew', 'secret', 'secret', 'kept', 'huge', 'secret', 'huge', 'secret', 'barber', 'kept', 'word', 'barber', 'kept', 'word', 'barber', 'kept', 'secret', 'keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy', 'barber', 'went', 'huge', 'mountain']\n"
     ]
    }
   ],
   "source": [
    "words = sum(sentences, []) # 1차원 리스트로 변환해주고\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3, 'word': 2, 'keeping': 2, 'good': 1, 'knew': 1, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1})\n"
     ]
    }
   ],
   "source": [
    "# Counter를 사용해 단어의 모든 빈도를 계산\n",
    "vocab = Counter(words)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(vocab['barber']) # 'barber'라는 단어의 빈도수를 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 빈도수가 높은 순서대로 정렬하고, 빈도 수가 높을 수록 낮은 수의 정수를 부여한다.\n",
    "\n",
    "- 이렇게 진행할 경우 빈도 수가 낮은 단어들을 제외시키면서 단어 집합의 크기를 조절하기가 편해진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)]\n"
     ]
    }
   ],
   "source": [
    "# 빈도수가 높은 순서대로 정렬\n",
    "vocab_sorted = sorted(vocab.items(), key = lambda x : x[1] , reverse = True)\n",
    "print(vocab_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\n"
     ]
    }
   ],
   "source": [
    "# 이제 높은 빈도수를 가진 단어일수록 낮은 정수 인덱스를 부여한다.\n",
    "word2idx = {}\n",
    "i = 0\n",
    "for (word,frequency) in vocab_sorted :\n",
    "    if frequency >  1 : # 빈도 수가 1일 경우 제외 \n",
    "        i = i + 1 # 인코딩 번호 +1\n",
    "        word2idx[word] = i\n",
    "print(word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**텍스트 데이터에 있는 단어를 모두 사용하기 보다 빈도수가 가장 높은 n개의 단어만 사용해야 할 때도 있다.**<br>\n",
    "ex) 네이버 영화 리뷰 20만개중 단어 집합이 37000개일 경우\n",
    "\n",
    "\n",
    "위 단어들은 빈도수가 높은 순서대로 낮은 정수가 부여되어 있으므로 빈도수 상위 n개의 단어만 사용하고 싶다면 vocab에서 정수값이 1부터 n까지인 단어들만 사용하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['word', 'keeping']\n",
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "words_frequency = [w for w,c in word2idx.items() if c >= vocab_size + 1] # 인덱스가 5 초과인 단어 제거\n",
    "print(words_frequency) \n",
    "for w in words_frequency : # 해당 단어들의 정보를 삭제\n",
    "    del word2idx[w]\n",
    "print(word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding\n",
    "\n",
    "- 모든 문장에 대해서 정수 인코딩을 수행했을 때 길이는 서로 다를 수 있다.\n",
    "\n",
    "- 이때 가상의 단어를 추가하여 길이를 맞춰준다.\n",
    "\n",
    "- 이렇게 하면 기계가 이를 병렬 연산할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One - Hot - Encoding\n",
    "\n",
    "- **원-핫 인코딩은 전체 단어 집합의 크기(중복은 카운트하지 않은 단어들의 개수)를 벡터의 차원으로 가진다.**\n",
    "\n",
    "- 각 단어에 고유한 정수 인덱스를 부여하고, 해당 인덱스의 원소는 1, 나머지 원소는 0을 가지는 벡터로 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Term Matrix - DTM\n",
    "\n",
    "- DTM은 마찬가지로 벡터가 단어 집합의 크기를 가지며, 대부분의 원소가 0을 가진다.\n",
    "\n",
    "- 각 단어는 고유한 정수 인덱스를 가지며, 해당 단어가 등장 횟루를 해당 인덱스의 값으로 가진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
