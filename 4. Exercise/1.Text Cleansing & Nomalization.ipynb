{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 처리는 전처리가 굉장히 중요하다.\n",
    "\n",
    "의미있는 내용을 얼마나 잘 필터링하는지에 따라 모델링의 결과가 좌우된다!\n",
    "\n",
    "따라서 재차 정리해보면서 다시 학습했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토큰화\n",
    "\n",
    "- 기계에게 어느 구간까지가 문장이고, 단어인지를 알려주어야 한다.\n",
    "\n",
    "- 문장 토큰화, 단어 토큰화, subword 토큰화 등. 다양한 단위의 토큰화가 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문장 내에서 단어를 어떻게 구분할까?\n",
    "\n",
    "- 만약 띄어쓰기로 단어를 구분한다면?\n",
    "\n",
    "We're Avengers!! -> ['we're', 'Avengers!!']\n",
    "\n",
    "We are Avengers!! -> ['we','are', 'Avengers!!']\n",
    "\n",
    "We are Avengers!! -> ['we','are', 'Avengers']\n",
    "\n",
    "각각 다른 의미의 3문장이 나온다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 특수 문자는 무조건 제거하고 본다?\n",
    "\n",
    "$45.55 --> 45 55 --> ['45','55']\n",
    "\n",
    "Ph.D --> Ph D --> ['Ph','D']\n",
    "\n",
    "본래 의미를 상실할 수 있다.\n",
    "\n",
    "#### --> 결국 상황에 맞는 전처리와 토크나이저의 선택이 성능을 좌우한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't를 Do와 n't로 분리하였으며, Jone's는 Jone과 's로 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK 토크나이저 2. WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "print(WordPunctTokenizer().tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't를 Don과 '와 t로 분리하였으며, Jone's를 Jone과 '와 s로 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK 토크나이저 3. TreebankWordTokenizer(eng)\n",
    "\n",
    "영어권 언어의 단어 토크나이저 중 하나인 TreebankWordTokenizer.\n",
    "\n",
    "표준 토큰화 규칙인 Penn Treebank Tokenization을 따른다.\n",
    "\n",
    "***\n",
    "**규칙1. 하이푼으로 구성된 단어는 하나로 유지한다.**\n",
    "\n",
    "**규칙2. dosen't와 같이 아포스트로피로 '접어'가 함께하는 단어로 분리해준다.**\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "text = \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문장 : KSS(Korean Sentence Splitter)\n",
    "\n",
    "한국어의 경우 문장 토큰화 패키지는 KSS 사용을 권장.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한국어는 조사, 접사 등으로 인해 단순 띄어쓰기 단위로 나누면 같은 단어가 다른 단어로 인식되는 경우가 너무 많기에 영어보다 까다롭다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'사과' == '사과의'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'사과의' == '사과가'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'사과를' == '사과의'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'사과' == '사과랑'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'의','를'과등이 붙어있으면 모두 다른 단어로 인식해버린다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 형태소 분석기 결과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import *\n",
    "\n",
    "han = Hannanum()\n",
    "kkma = Kkma()\n",
    "komoran = Komoran()\n",
    "okt = Okt()\n",
    "mecab = Mecab('C:\\mecab\\mecab-ko-dic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 형태소 분석기들은 공통적으로 아래의 함수를 제공한다.\n",
    "\n",
    "nouns : 명사 추출\n",
    "\n",
    "morphs : 형태소 추출\n",
    "\n",
    "pos : 품사 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['코딩', '당신', '연휴', '휴가']\n",
      "['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '휴가', '를', '가봐요']\n",
      "[('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('휴가', 'Noun'), ('를', 'Josa'), ('가봐요', 'Verb')]\n"
     ]
    }
   ],
   "source": [
    "print(okt.nouns(\"열심히 코딩한 당신, 연휴에는 휴가를 가봐요\"))\n",
    "print(okt.morphs(\"열심히 코딩한 당신, 연휴에는 휴가를 가봐요\")) # morphs가 전형적인 단어를 토큰화하는 작업\n",
    "print(okt.pos(\"열심히 코딩한 당신, 연휴에는 휴가를 가봐요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['코딩', '당신', '연휴', '휴가']\n",
      "['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '휴가', '를', '가보', '아요']\n",
      "[('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('휴가', 'NNG'), ('를', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')]\n"
     ]
    }
   ],
   "source": [
    "print(kkma.nouns(\"열심히 코딩한 당신, 연휴에는 휴가를 가봐요\"))\n",
    "print(kkma.morphs(\"열심히 코딩한 당신, 연휴에는 휴가를 가봐요\"))\n",
    "print(kkma.pos(\"열심히 코딩한 당신, 연휴에는 휴가를 가봐요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['코', '당신', '연휴', '휴가']\n",
      "['열심히', '코', '딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '휴가', '를', '가', '아', '보', '아요']\n",
      "[('열심히', 'MAG'), ('코', 'NNG'), ('딩', 'MAG'), ('하', 'XSV'), ('ㄴ', 'ETM'), ('당신', 'NNP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKB'), ('는', 'JX'), ('휴가', 'NNG'), ('를', 'JKO'), ('가', 'VV'), ('아', 'EC'), ('보', 'VX'), ('아요', 'EC')]\n"
     ]
    }
   ],
   "source": [
    "print(komoran.nouns(\"열심히 코딩한 당신, 연휴에는 휴가를 가봐요\"))\n",
    "print(komoran.morphs(\"열심히 코딩한 당신, 연휴에는 휴가를 가봐요\"))\n",
    "print(komoran.pos(\"열심히 코딩한 당신, 연휴에는 휴가를 가봐요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['코딩', '당신', '연휴', '휴가']\n",
      "['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에는', '휴가', '를', '가', '아', '보', '아']\n",
      "[('열심히', 'M'), ('코딩', 'N'), ('하', 'X'), ('ㄴ', 'E'), ('당신', 'N'), (',', 'S'), ('연휴', 'N'), ('에는', 'J'), ('휴가', 'N'), ('를', 'J'), ('가', 'P'), ('아', 'E'), ('보', 'P'), ('아', 'E')]\n"
     ]
    }
   ],
   "source": [
    "print(han.nouns(\"열심히 코딩한 당신, 연휴에는 휴가를 가봐요\"))\n",
    "print(han.morphs(\"열심히 코딩한 당신, 연휴에는 휴가를 가봐요\"))\n",
    "print(han.pos(\"열심히 코딩한 당신, 연휴에는 휴가를 가봐요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['코딩', '당신', '연휴', '휴가']\n",
      "['열심히', '코딩', '한', '당신', ',', '연휴', '에', '는', '휴가', '를', '가', '봐요']\n",
      "[('열심히', 'MAG'), ('코딩', 'NNG'), ('한', 'XSA+ETM'), ('당신', 'NP'), (',', 'SC'), ('연휴', 'NNG'), ('에', 'JKB'), ('는', 'JX'), ('휴가', 'NNG'), ('를', 'JKO'), ('가', 'VV'), ('봐요', 'EC+VX+EC')]\n"
     ]
    }
   ],
   "source": [
    "print(mecab.nouns(\"열심히 코딩한 당신, 연휴에는 휴가를 가봐요\"))\n",
    "print(mecab.morphs(\"열심히 코딩한 당신, 연휴에는 휴가를 가봐요\"))\n",
    "print(mecab.pos(\"열심히 코딩한 당신, 연휴에는 휴가를 가봐요\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한국어 Sentence Tokenization(KSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kss\n",
    "text = '딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어려워요. 이제 해보면 알걸요?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어려워요.', '이제 해보면 알걸요?']\n"
     ]
    }
   ],
   "source": [
    "print(kss.split_sentences(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한국어 띄어쓰기 패키지(pykospacing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임의의 문장 생성\n",
    "sent = \"비트코인은 단기 저점을 찍고 1억 반등에 성공할 것이다. 일론 머스크와 투더문 가즈아~!! (이상 현재 관망중)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "비트코인은 단기 저점을 찍고 1억 반등에 성공할 것이다. 일론 머스크와 투더문 가즈아~!! (이상 현재 관망중)\n"
     ]
    }
   ],
   "source": [
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "비트코인은단기저점을찍고1억반등에성공할것이다.일론머스크와투더문가즈아~!!(이상현재관망중)\n"
     ]
    }
   ],
   "source": [
    "# 띄어쓰기 제거\n",
    "new_sent = sent.replace(' ','')\n",
    "print(new_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "비트코인은 단기 저점을 찍고 1억 반등에 성공할 것이다. 일론 머스크와 투더문 가즈아~!! (이상 현재 관망중)\n",
      "비트코인은 단기 저점을 찍고 1억 반등에 성공할 것이다.일론 머스크와 투더문 가즈아~!!(이상 현재 관망중)\n"
     ]
    }
   ],
   "source": [
    "from pykospacing import Spacing\n",
    "spacing = Spacing()\n",
    "kospacing_sent = spacing(new_sent) \n",
    "print(sent)\n",
    "print(kospacing_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "띄어쓰기 보정이 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and Normalization\n",
    "\n",
    "### Cleaning(정제) : 불필요한 데이터를 제거하는 일\n",
    "- 정규표현식을 이용한 노이즈 데이터 제거\n",
    "- 인코딩 문제 해결\n",
    "- 등장 빈도가 적은 단어 제거 : 등장 빈도가 2회 이하\n",
    "- 길이가 짧은 단어 제거(영어의 경우) : I, by, at 등...\n",
    "- 불용어 제거 : I, at, for, by, 은, 는, 이, 가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정규 표현식을 이용한 정제의 예"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = '와 이런 것도 영화라고...ㅋㅋㅋㅋ 차라리 든든한 국밥 한그릇 하는게 돈 아꼈다ㅋㅋㅋ 진짜ㅏ 이 감독 믿고 거를란다~'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'와 이런 것도 영화라고...ㅋㅋㅋㅋ 차라리 든든한 국밥 한그릇 하는게 돈 아꼈다ㅋㅋㅋ 진짜ㅏ 이 감독 믿고 거를란다~'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 한국어를 제외하고 모두 제거\n",
    "new_sent = re.sub(\"[^ㄱ-ㅎ ㅏ-ㅣ 가-힣]\",\"\",sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'와 이런 것도 영화라고ㅋㅋㅋㅋ 차라리 든든한 국밥 한그릇 하는게 돈 아꼈다ㅋㅋㅋ 진짜ㅏ 이 감독 믿고 거를란다'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 완성형 한글만 남기는 경우\n",
    "new_sent = re.sub(\"[^가-힣]\",\" \",sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'와 이런 것도 영화라고        차라리 든든한 국밥 한그릇 하는게 돈 아꼈다    진짜  이 감독 믿고 거를란다 '"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'와 이런 것도 영화라고 차라리 든든한 국밥 한그릇 하는게 돈 아꼈다 진짜 이 감독 믿고 거를란다 '"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 띄어쓰기가 반복될 경우 1개로 줄인다.\n",
    "kor_sent = re.sub(\"[ ]{2,}\",\" \",new_sent)\n",
    "kor_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정규표현식은 자연어 처리를 하고자 한다면 필수불가결이다.외우진 못하더라도 필요하다면 검색해서라도 응용하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "- 다르게 표기된 단어들을 하나로 통합(Normalization)\n",
    "\n",
    "Apples<br> \n",
    "Apple <br>\n",
    "apples<br>\n",
    "**apple로 통합**\n",
    "\n",
    "걸쳐<br> \n",
    "걸쳐서 <br>\n",
    "걸치면서<br>\n",
    "**걸치다로 통합**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization(kor)\n",
    "\n",
    "- 다르게 표기된 문자들을 하나로 통합\n",
    "\n",
    "- 정규 표현식 또는 soynlp.normalizer를 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soynlp.normalizer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ㅋㅋ, ㅎㅎ등의 이모티콘의 경우 불필요하게 연속되는 경우가 많은데, 이에 반복되는 것은 하나로 통일해줄수가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아ㅋㅋ영화존잼쓰ㅠㅠ\n",
      "아ㅋㅋ영화존잼쓰ㅠㅠ\n",
      "아ㅋㅋ영화존잼쓰ㅠㅠ\n",
      "아ㅋㅋ영화존잼쓰ㅠㅠ\n"
     ]
    }
   ],
   "source": [
    "print(emoticon_normalize('앜ㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠ', num_repeats=2))\n",
    "print(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠ', num_repeats=2))\n",
    "print(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠㅠ', num_repeats=2))\n",
    "print(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠㅠㅠㅠ', num_repeats=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 맞춤법 검사기(Kor)\n",
    "\n",
    "- hanspell.spell.checker는 맞춤법 및 띄어쓰기 정정 기능 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "맞춤법 틀리면 왜 안돼?\n"
     ]
    }
   ],
   "source": [
    "# 맞춤법 교정\n",
    "from hanspell import spell_checker\n",
    "sent = '맞춤법 틀리면 외 않되?'\n",
    "spelled_send = spell_checker.check(sent)\n",
    "hanspell_sent = spelled_send.checked\n",
    "print(hanspell_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "비트코인은 단기 저점을 찍고 1억 반등에 성공할 것이다. 일론 머스크와 투 더 문 가즈에~!! (이상 현재 관망 중)\n",
      "비트코인은 단기 저점을 찍고 1억 반등에 성공할 것이다.일론 머스크와 투더문 가즈아~!!(이상 현재 관망중)\n"
     ]
    }
   ],
   "source": [
    "# 띄어쓰기 교정\n",
    "sent = \"비트코인은 단기 저점을 찍고 1억 반등에 성공할 것이다. 일론 머스크와 투더문 가즈아~!! (이상 현재 관망중)\"\n",
    "spelled_send = spell_checker.check(sent)\n",
    "hanspell_sent = spelled_send.checked\n",
    "print(hanspell_sent)\n",
    "print(kospacing_sent) # kospacing으로 얻은 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 영어 : Stemming\n",
    "- 어간(stem)을 추출하는 작업을 어간 추출(stemming)이라고 한다.\n",
    "\n",
    "정해진 규칙만 보고 단어의 어미를 자르는 어림짐작의 작업이라고 볼 수 있다.\n",
    "\n",
    "다시 말해, 이 작업은 섬세한 작업이 아니기 때문에 어간 추출에 나오는 결과 단어는 사전에 존재하지 않는 단어일 수도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가령, 포터 스테머의 포터 알고리즘의 어간 추출은 이러한 규칙들을 가진다.  \n",
    "ALIZE → AL  \n",
    "ANCE → 제거  \n",
    "ICAL → IC  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "porterstemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['formal', 'allow', 'electric']\n"
     ]
    }
   ],
   "source": [
    "# 규칙에 의한 추출\n",
    "words = ['formalize','allowance','electricical']\n",
    "print([porterstemmer.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한국어 : Stemming and Normalization(Okt)\n",
    "\n",
    "- Okt에서도 Stemming 옵션을 제공한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '나는 6시가 땡치면 회사를 나와 집으로 갑니다'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나', '는', '6시', '가', '땡', '치면', '회사', '를', '나와', '집', '으로', '갑니다']\n",
      "['나', '는', '6시', '가', '땡', '치면', '회사', '를', '나오다', '집', '으로', '가다']\n",
      "[('나', 'Noun'), ('는', 'Josa'), ('6시', 'Number'), ('가', 'Foreign'), ('땡', 'Noun'), ('치면', 'Noun'), ('회사', 'Noun'), ('를', 'Josa'), ('나와', 'Verb'), ('집', 'Noun'), ('으로', 'Josa'), ('갑니다', 'Verb')]\n",
      "[('나', 'Noun'), ('는', 'Josa'), ('6시', 'Number'), ('가', 'Foreign'), ('땡', 'Noun'), ('치면', 'Noun'), ('회사', 'Noun'), ('를', 'Josa'), ('나오다', 'Verb'), ('집', 'Noun'), ('으로', 'Josa'), ('가다', 'Verb')]\n"
     ]
    }
   ],
   "source": [
    "# Okt stem = True 추가 비교\n",
    "print(okt.morphs(text))\n",
    "print(okt.morphs(text,stem = True))\n",
    "print(okt.pos(text))\n",
    "print(okt.pos(text,stem = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['웃기는', '소리', '하지마', '랔', 'ㅋㅋㅋㅋ']\n",
      "['웃기는', '소리', '하지마라', 'ㅋㅋㅋ']\n",
      "[('웃기는', 'Verb'), ('소리', 'Noun'), ('하지마', 'Verb'), ('랔', 'Noun'), ('ㅋㅋㅋㅋ', 'KoreanParticle')]\n",
      "[('웃기는', 'Verb'), ('소리', 'Noun'), ('하지마라', 'Verb'), ('ㅋㅋㅋ', 'KoreanParticle')]\n"
     ]
    }
   ],
   "source": [
    "# norm = True로 사용시 '랔'과 같은 단어를 이모티콘으로 취급한다.\n",
    "text = '웃기는 소리하지마랔ㅋㅋㅋㅋ'\n",
    "print(okt.morphs(text))\n",
    "print(okt.morphs(text,norm = True))\n",
    "print(okt.pos(text))\n",
    "print(okt.pos(text,norm = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 불용어 제거\n",
    "\n",
    "- 자주 등장하지만 실제 분석을 하는데는 거의 기여하는 바가 없는 단어들.\n",
    "\n",
    "\n",
    "- 영어의 경우 'the'는 거의 모든 텍스트 데이터에서 등장 빈도수가 가장 높은 단어지만 실제 의미는 갖고있지 않으므로 대표적인 불용어\n",
    "\n",
    "\n",
    "- NLTK에서 정의한 영어 불용어로는 'i','me','my','myself','we','our','ours','you'등이 존재한다.\n",
    "\n",
    "\n",
    "- 한국어의 경우 '그럼','위하','때','가','어떤','은','를'등이 존재한다.\n",
    "\n",
    "\n",
    "- 불용어는 절대적인 기준이 아니라 어떤 데이터인지, 어떤 문제를 푸는지, 어떤 토크나이저를 사용하는지에 따라서 달라진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 영어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Family is not an important thing. It's everything.\n"
     ]
    }
   ],
   "source": [
    "example = \"Family is not an important thing. It's everything.\"\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 :  ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
      "불용어 제거후 : ['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords  \n",
    "sentences = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "result = []\n",
    "for w in word_tokens :\n",
    "    if w not in stop_words :\n",
    "        result.append(w)\n",
    "        \n",
    "print('원문 : ', word_tokens)\n",
    "print(\"불용어 제거후 :\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한국어\n",
    "\n",
    "- 영어는 단어 토큰화를 진행한 후 불용어 제거\n",
    "\n",
    "- 한글도 마찬가지다. 대신 **한글은 형태소 분석기를 정한 다음에 불용어를 제거하자.**\n",
    "\n",
    "#### -> 어떻게 나눠지는지를 보고 불용어를 제거할 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['와', '이런', '것', '도', '영화', '라고', '차라리', '든든한', '국밥', '한', '그릇', '하는게', '돈', '아꼈다', '진짜', '이', '감독', '믿고', '거', '를', '란', '다']\n"
     ]
    }
   ],
   "source": [
    "text = '와 이런 것도 영화라고 차라리 든든한 국밥 한그릇 하는게 돈 아꼈다 진짜 이 감독 믿고 거를란다'\n",
    "\n",
    "word_tokens = okt.morphs(text)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','진짜','으로','자','에','와','한','하다','것','를']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 처리 전 ['와', '이런', '것', '도', '영화', '라고', '차라리', '든든', '한', '국밥', '한', '그릇', '하', '는', '게', '돈', '아꼈', '다', '진짜', '이', '감독', '믿', '고', '거', '를', '란다']\n",
      "불용어 처리 후 ['이런', '영화', '라고', '차라리', '든든', '국밥', '그릇', '하', '게', '돈', '아꼈', '다', '감독', '믿', '고', '거', '란다'] "
     ]
    }
   ],
   "source": [
    "result = [word for word in word_tokens if word not in stop_words]\n",
    "print(\"불용어 처리 전\",word_tokens)\n",
    "print(\"불용어 처리 후\",result, end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['와', '이런', '것', '도', '영화', '라고', '차라리', '든든', '한', '국밥', '한', '그릇', '하', '는', '게', '돈', '아꼈', '다', '진짜', '이', '감독', '믿', '고', '거', '를', '란다']\n"
     ]
    }
   ],
   "source": [
    "# mecab으로도 실행\n",
    "text = '와 이런 것도 영화라고 차라리 든든한 국밥 한그릇 하는게 돈 아꼈다 진짜 이 감독 믿고 거를란다'\n",
    "\n",
    "word_tokens = mecab.morphs(text)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 처리 전 ['와', '이런', '것', '도', '영화', '라고', '차라리', '든든', '한', '국밥', '한', '그릇', '하', '는', '게', '돈', '아꼈', '다', '진짜', '이', '감독', '믿', '고', '거', '를', '란다']\n",
      "불용어 처리 후 ['이런', '영화', '라고', '차라리', '든든', '국밥', '그릇', '하', '게', '돈', '아꼈', '다', '감독', '믿', '고', '거', '란다'] "
     ]
    }
   ],
   "source": [
    "result = [word for word in word_tokens if word not in stop_words]\n",
    "print(\"불용어 처리 전\",word_tokens)\n",
    "print(\"불용어 처리 후\",result, end = ' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
